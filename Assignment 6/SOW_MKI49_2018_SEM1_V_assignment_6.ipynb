{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "hZ0qCE9k_YLL"
   },
   "source": [
    "In this assignment you will learn how to apply the REINFORCE algorithm within the OpenAI Gym environment. Make sure OpenAI gym is installed on your machine. Now let's import some relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nW43sE3G_YLN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Djamari\\Anaconda2\\envs\\py3k6\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers, logger\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import Chain\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer.optimizers import Adam\n",
    "from chainer import Variable\n",
    "import pdb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4NkXTUs_YLQ"
   },
   "source": [
    "We will make use of the classic CartPole environment provided by OpenAI Gym. Figure out what the details of this environment are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y5Ge90x1_YLR"
   },
   "outputs": [],
   "source": [
    "env_id = 'CartPole-v0'\n",
    "\n",
    "# You can set the level to logger.DEBUG or logger.WARN if you want to change the amount of output.\n",
    "logger.set_level(logger.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "akhOk1Dq_YLU"
   },
   "source": [
    "Let's define a baseline agent which just emits random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ch4QR3vI_YLU"
   },
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZCSE-aO_YLX"
   },
   "source": [
    "Let's run the agent on the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSQ2wNlh_YLY",
    "outputId": "eceee47f-5300-4f0c-abfb-858fcfb64bbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3704.14it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_id)\n",
    "env.seed(0)\n",
    "agent = RandomAgent(env.action_space)\n",
    "\n",
    "episode_count = 1000\n",
    "done = False\n",
    "reward = 0\n",
    "    \n",
    "R0 = np.zeros(episode_count)\n",
    "for i in tqdm.trange(episode_count):\n",
    "\n",
    "    ob = env.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        action = agent.act(ob, reward, done)\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "\n",
    "        R0[i] += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# Close the env and write monitor result info to disk\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R20rGDxh_YLc"
   },
   "source": [
    "Let's create the REINFORCE agent. We assume that the policy is computed using an MLP with a softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AFUpzrbm_YLd"
   },
   "outputs": [],
   "source": [
    "class MLP(Chain):\n",
    "    \"\"\"Multilayer perceptron\"\"\"\n",
    "\n",
    "    def __init__(self, n_output=1, n_hidden=5):\n",
    "        super(MLP, self).__init__(l1=L.Linear(None, n_hidden), l2=L.Linear(n_hidden, n_output))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VuNVcrZE_YLg"
   },
   "source": [
    "1: A skeleton for the REINFORCEAgent is given. Implement the compute_loss and compute_score functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cUkMCjXg_YLg"
   },
   "outputs": [],
   "source": [
    "class REINFORCEAgent(object):\n",
    "    \"\"\"Agent trained using REINFORCE\"\"\"\n",
    "\n",
    "    def __init__(self, action_space, model, optimizer=Adam()):\n",
    "\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer.setup(self.model)\n",
    "\n",
    "        # monitor score and reward\n",
    "        self.rewards = []\n",
    "        self.scores = []\n",
    "\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "\n",
    "        # linear outputs reflecting the log action probabilities and the value\n",
    "        policy = self.model(Variable(np.atleast_2d(np.asarray(observation, 'float32'))))\n",
    "\n",
    "        # generate action according to policy\n",
    "        p = F.softmax(policy).data\n",
    "\n",
    "        # normalize p in case tiny floating precision problems occur\n",
    "        row_sums = p.sum(axis=1)\n",
    "        p /= row_sums[:, np.newaxis]\n",
    "\n",
    "        action = np.asarray([np.random.choice(p.shape[1], None, True, p[0])])\n",
    "\n",
    "        return action, policy\n",
    "\n",
    "    # refer to slide 23, last formula\n",
    "    def compute_loss(self):\n",
    "        \"\"\"\n",
    "        Return loss for this episode based on computed scores and accumulated rewards\n",
    "        \"\"\"   \n",
    "        #print(self.rewards, self.scores)\n",
    "        #pdb.set_trace()\n",
    "        J = 1/(i + 1) * sum(np.log(np.multiply(np.asarray(self.scores), np.asarray(self.rewards))))\n",
    "        J_ = np.array(J)\n",
    "        return chainer.as_variable(J_)       \n",
    "        \n",
    "\n",
    "    # refer to slide 30\n",
    "    # grad_theta log pi_theta (s_t, a_t) * v_t\n",
    "    def compute_score(self, action, policy):\n",
    "        \"\"\"\n",
    "        Computes score\n",
    "\n",
    "        Args:\n",
    "            action (int):\n",
    "            policy:\n",
    "\n",
    "        Returns:\n",
    "            score\n",
    "        \"\"\"\n",
    "     \n",
    "        \n",
    "        return (F.log(F.softmax(policy)[0,action[0]])).array\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3nXDQui_YLi"
   },
   "source": [
    "Now we run the REINFORCE agent on the CartPole environment. Note that we update the agent after each episode for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSDra-S__YLk",
    "outputId": "51a61d7d-22e0-4e5d-e7ed-4e773b6ea2da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]C:\\Users\\Djamari\\Anaconda2\\envs\\py3k6\\lib\\site-packages\\ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in log\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 92.62it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_id)\n",
    "env.seed(0)\n",
    "\n",
    "network = MLP(n_output=env.action_space.n, n_hidden=3)\n",
    "agent = REINFORCEAgent(env.action_space, network, optimizer=Adam())\n",
    "\n",
    "episode_count = 1000\n",
    "done = False\n",
    "reward = 0\n",
    "    \n",
    "R = np.zeros(episode_count)\n",
    "for i in tqdm.trange(episode_count):\n",
    "\n",
    "    ob = env.reset()\n",
    "\n",
    "    loss = 0\n",
    "    while True:\n",
    "\n",
    "        action, policy = agent.act(ob, reward, done)\n",
    "\n",
    "        ob, reward, done, _ = env.step(action[0])\n",
    "\n",
    "        # get reward associated with taking the previous action in the previous state\n",
    "        agent.rewards.append(reward)\n",
    "        R[i] += reward\n",
    "\n",
    "        # recompute score function: grad_theta log pi_theta (s_t, a_t) * v_t\n",
    "        agent.scores.append(agent.compute_score(action, policy).item())\n",
    "        #print(type(agent.compute_score(action, policy)))\n",
    "        # we learn at the end of each episode\n",
    "        if done:\n",
    "            \n",
    "            loss += agent.compute_loss()\n",
    "            \n",
    "            agent.model.cleargrads()\n",
    "            loss.backward()\n",
    "            loss.unchain_backward()\n",
    "            agent.optimizer.update()\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tRgTVYvq_YLo"
   },
   "outputs": [],
   "source": [
    "#You may want to run a video of the trained agent performing in the environment using the env.render() function.\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    ob = env.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        action, policy = agent.act(ob, reward, done)\n",
    "\n",
    "        ob, reward, done, _ = env.step(action[0])\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "      \n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shHGeCVz_YLr"
   },
   "source": [
    "2: Plot the cumulative reward for both RandomAgent and REINFORCEAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SOW-MKI49-2018-SEM1-V-assignment_6.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
