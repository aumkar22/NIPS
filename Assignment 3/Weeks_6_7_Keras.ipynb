{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras import backend as k\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.engine.topology import Layer\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "n_z = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(inputs):\n",
    "\n",
    "    encode = Conv2D(32, (2, 2), strides=2)(inputs)\n",
    "\n",
    "    encode = MaxPooling2D((2, 2))(encode)\n",
    "\n",
    "    encode = Activation('relu')(encode)\n",
    "\n",
    "    encode = Conv2D(32, (2, 2), strides=2)(encode)\n",
    "\n",
    "    encode = MaxPooling2D((2, 2))(encode)\n",
    "\n",
    "    encode = Activation('relu')(encode)\n",
    "\n",
    "    encode = Flatten()(encode)\n",
    "\n",
    "    encode = Dense(512, activation = 'relu')(encode)\n",
    "    \n",
    "    encode = Dropout(0.5)(encode)\n",
    "\n",
    "    mu = Dense(n_z, activation = 'linear')(encode)\n",
    "\n",
    "    log_sigma = Dense(n_z, activation = 'linear')(encode)\n",
    "    \n",
    "    return mu, log_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(latent_vector): \n",
    "    \n",
    "    decode = Dense(512, activation = 'relu')(latent_vector)\n",
    "    \n",
    "    decode = Reshape((14, 14, 1))(decode)\n",
    "    \n",
    "    decode = Conv2D(32, (2, 2), strides=2)(decode)\n",
    "    \n",
    "    decode = UpSampling2D((2, 2))(decode)\n",
    "    \n",
    "    decode = Activation('relu')(decode)\n",
    "    \n",
    "    decode = Conv2D(32, (2, 2), strides=2)(decode)\n",
    "    \n",
    "    decode = UpSampling2D((2, 2))(decode)\n",
    "    \n",
    "    decode = Activation('relu')(decode)\n",
    "    \n",
    "    out = Conv2D(1, (2, 2), activation = 'sigmoid')(decode)\n",
    "    \n",
    "    mu_out = Dense(n_z, activation = 'linear')(out)\n",
    "\n",
    "    log_sigma_out = Dense(n_z, activation = 'linear')(out)\n",
    "    \n",
    "    return mu_out, log_sigma_out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss():\n",
    "    # p_z = gaussian(0,1)\n",
    "    # q_z_x = gaussian(mean_y, e^ln_var_y) - check this exp\n",
    "    # p_x_z = gaissian(mean_x, e^ln_var_x)\n",
    "    # L = F.gaussian_kl_dvergance(q_z_x, p_z) - F.gaussian_nll(p_x_z)\n",
    "        \n",
    "    #? What about input x?\n",
    "        \n",
    "    # Why use log variance instead of variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_z(mu, log_sigma):\n",
    "    eps = K.random_normal(shape=(mu, n_z), mean=0., stddev=1.)\n",
    "    return mu + K.exp(log_sigma / 2) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "batches = int(len(x_train)/batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator():\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        for batch in range(batches):\n",
    "            x = []\n",
    "            \n",
    "            data = x_train[batch * batchsize: (batch + 1) * batchsize]\n",
    "            for i in data:\n",
    "                x.append(np.reshape(i, (28, 28, 1)))\n",
    "            \n",
    "            x_train1 = np.asarray(x)\n",
    "            yield (x_train1, x_train1) # the data is also the target (since we're re-generating images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs1 = Input(shape = (28, 28, 1))\n",
    "\n",
    "mu, log_sigma = encoder(inputs1)\n",
    "\n",
    "latent_vector = sample_z(mu, log_sigma)\n",
    "\n",
    "mu_out, log_sigma_out, out_img = decoder(latent_vector)\n",
    "\n",
    "model = Model(inputs = input1, outputs = out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(batch_generator(), steps_per_epoch = batches, epochs=10, shuffle = True, callbacks = callback_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
