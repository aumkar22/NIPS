{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import time\n",
    "from keras import backend as k\n",
    "from keras import optimizers\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.engine.topology import Layer\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import binary_crossentropy\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchsize = 2\n",
    "n_z = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the sample function needed to combine the encoder and decoder model.\n",
    "\n",
    "Task: (20 points)\n",
    "\n",
    "- Implement the reparameterziation trick for sampling latents. (10 points)\n",
    "- Explain why we need to use this trick. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_z(args):\n",
    "    mu, log_sigma = args\n",
    "    batch = K.shape(mu)[0]\n",
    "    dim = K.int_shape(mu)[1]\n",
    "    eps = K.random_normal(shape=(batch, dim), mean=0., stddev=1.)\n",
    "    return mu + K.exp(log_sigma / 2) * eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain why we need to use this trick:**\n",
    "\n",
    "To train the model, we need the gradient of the sampling operation, but without the reparametrization trick, this samping operation is not differentiable. This trick takes the part that is not differentiable out of the network which enables us to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#batches = int(len(x_train)/batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train1 = x_train[:20000]\n",
    "batches = int(len(x_train1)/batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_val = np.reshape(x_test[:5000], (x_test[:5000].shape[0], 28, 28, 1))\n",
    "x_test1 = np.reshape(x_test[5000:], (x_test[5000:].shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator():\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        for batch in range(batches):\n",
    "            x = []\n",
    "            \n",
    "            data = x_train1[batch * batchsize: (batch + 1) * batchsize]\n",
    "            for i in data:\n",
    "                x.append(np.reshape(i, (28, 28, 1)))\n",
    "            \n",
    "            x_train2 = np.asarray(x)\n",
    "            yield (x_train2, x_train2) # the data is also the target (since we're re-generating images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for the encoder. It transforms observables (images) to latents (features). It corresponds to q(z | x) in the context of variational inference (and the slides), where z is latents and x is observables.\n",
    "\n",
    "Task: (10 points)\n",
    "\n",
    "- Implement the encoder class for a variational autoencoder. Note that the encoder should output the Gaussian distribution parameters (mean and variance per feature) of features rather than features themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs1 = Input(shape = (28, 28, 1))\n",
    "\n",
    "encode = Conv2D(32, (2, 2), strides=2, padding = 'same')(inputs1)\n",
    "\n",
    "encode = Activation('relu')(encode)\n",
    "\n",
    "encode = Conv2D(32, (2, 2), strides=2, padding = 'same')(encode)\n",
    "\n",
    "encode = Activation('relu')(encode)\n",
    "\n",
    "out_shape = list(k.int_shape(encode))\n",
    "\n",
    "encode = Flatten()(encode)\n",
    "\n",
    "encode = Dense(520, activation = 'relu')(encode)\n",
    "\n",
    "encode = Dropout(0.5)(encode)\n",
    "\n",
    "mu = Dense(n_z, activation = 'linear')(encode)\n",
    "\n",
    "log_sigma = Dense(n_z, activation = 'linear')(encode)\n",
    "\n",
    "latent_vector = Lambda(sample_z, output_shape=(n_z,))([mu, log_sigma])\n",
    "\n",
    "model_encoder = Model(inputs = inputs1, outputs = [mu, log_sigma, latent_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 14, 14, 32)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 14, 14, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 32)     4128        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 7, 7, 32)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1568)         0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 520)          815880      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 520)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            1042        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            1042        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2)            0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 822,252\n",
      "Trainable params: 822,252\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for the decoder. It transforms latents (features) to observables (images). It corresponds to p(x | z) in the context of variational inference (and the slides), where x is observables and z is latents.\n",
    "\n",
    "Task: (10 points)\n",
    "\n",
    "- Implement the decoder class for a variational autoencoder. Note that the decoder should output the Gaussian distribution parameters (mean and variance per pixel) of images rather than images themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs2 = Input(shape = (n_z,))\n",
    "\n",
    "decode = Dense(out_shape[1] * out_shape[2] * out_shape[3], activation = 'relu')(inputs2)\n",
    "\n",
    "decode = Reshape((out_shape[1], out_shape[2], out_shape[3]))(decode)\n",
    "\n",
    "decode = Conv2DTranspose(32, (2, 2), strides=2, padding = 'same')(decode)\n",
    "\n",
    "decode = Activation('relu')(decode)\n",
    "\n",
    "decode = Conv2DTranspose(32, (2, 2), strides=2, padding = 'same')(decode)\n",
    "\n",
    "decode = Activation('relu')(decode)\n",
    "\n",
    "out = Conv2DTranspose(1, (2, 2), activation = 'sigmoid', padding = 'same')(decode)\n",
    "\n",
    "model_decoder = Model(inputs = inputs2, outputs = out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1568)              4704      \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 32)        4128      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 32)        4128      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         129       \n",
      "=================================================================\n",
      "Total params: 13,089\n",
      "Trainable params: 13,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs = model_decoder(model_encoder(inputs1)[2])\n",
    "model = Model(inputs1, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the loss class. The loss of encoder and decoder of a variational autoencoder is the evidence lower bound as follows:\n",
    "\n",
    "$L = D_{KL}(q(z | x), p(z)) -  E_{z\\sim q}[log p(x | z)]$\n",
    "\n",
    "The first term above is the KL divergence between the approximate posterior (q) and the prior (p), which can be interpreted as a form of regularization. You can assume that the prior is unit Gaussian. It can be implemented with the F.gaussian_kl_divergence function in Chainer.\n",
    "\n",
    "The second term above is the Gaussian negative log likelihood. This is the term that fits the data, which is very similar to the usual loss functions that you use in deep learning. It can be implemented with the F.gaussian_nll function in Chainer.\n",
    "\n",
    "Task: \n",
    "\n",
    "- Implement the loss class. (10 points)\n",
    "- Explain why we use log variance instead of variance. (5 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _loss(y_true, y_pred):\n",
    "    reconstruction_loss = image_size * image_size * binary_crossentropy(K.flatten(y_true), K.flatten(y_pred))\n",
    "    kl_loss = -0.5 * K.sum(1 + log_sigma - K.square(mu) - K.exp(log_sigma), axis=-1)\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    \n",
    "    return vae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain why we use log variance instead of variance:**\n",
    "\n",
    "Why use log variance instead of variance: Because taking the exponent is numerically more stable than taking the log.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam_ = optimizers.Adam(lr = 1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = adam_, loss = _loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='checkpoint_vae.h5', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"./vae_logs/{}\", histogram_freq = 1, write_graph=True, \n",
    "                          write_images = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callback_list = [tensorboard, checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1314/10000 [==>...........................] - ETA: 11:39 - loss: -145971.6284"
     ]
    }
   ],
   "source": [
    "model.fit_generator(batch_generator(), steps_per_epoch = batches, epochs = 10, shuffle = True, \n",
    "                    validation_data = (x_val, x_val), callbacks = callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('vae.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_encoder.save('encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_decoder.save('decoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = load_model('vae.h5', custom_objects = {'_loss': _loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: (50 points)\n",
    "\n",
    "- Train the above defined variational autoencoder on the Mnist dataset. You can refer to the earlier assignments to implement your training loop. (25 points)\n",
    "\n",
    "- How good are the samples? Randomy sample some digits and visualize them. (10 points)\n",
    "\n",
    "- How good are the reconstructions? Draw an Mnist like digit, encode it, decode it and visualize the digits. How different is the reconstruction from the original? (10 points)\n",
    "\n",
    "- Repeat the last task but by drawing something other than a digit (e.g., a face). How accuracte is the reconstructions? Explain the results. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** How good are the samples? Randomy sample some digits and visualize them. (10 points) **\n",
    "\n",
    "As it can be seen the samples are good in a sense that you can recognize the digits displayed. However the overall quality of the samples can still be argued to be quite bad, since the images are very pixelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    random_number = np.random.randint(0, x_train.shape[0])\n",
    "    imshow(x_train[random_number, :, :], cmap=cm.binary)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_mean, _, _ = model_encoder.predict(x_test1, batch_size=batchsize)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"z[0]\")\n",
    "plt.ylabel(\"z[1]\")\n",
    "#plt.savefig(filename)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "digit_size = 28\n",
    "\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# linearly spaced coordinates corresponding to the 2D plot\n",
    "# of digit classes in the latent space\n",
    "grid_x = np.linspace(-4, 4, n)\n",
    "grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "for i, yi in enumerate(grid_y):\n",
    "    for j, xi in enumerate(grid_x):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = model_decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "start_range = digit_size // 2\n",
    "end_range = n * digit_size + start_range + 1\n",
    "pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "sample_range_x = np.round(grid_x, 1)\n",
    "sample_range_y = np.round(grid_y, 1)\n",
    "plt.xticks(pixel_range, sample_range_x)\n",
    "plt.yticks(pixel_range, sample_range_y)\n",
    "plt.xlabel(\"z[0]\")\n",
    "plt.ylabel(\"z[1]\")\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "#plt.savefig(filename)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
