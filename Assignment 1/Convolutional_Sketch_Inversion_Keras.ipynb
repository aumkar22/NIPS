{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras import backend as k\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.engine.topology import Layer\n",
    "from PIL import Image\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "batch_test = 4\n",
    "root_dir = os.path.normpath(os.path.join(os.path.dirname(os.path.realpath('__file__'))))\n",
    "data_directory = os.path.join(root_dir, 'lfw-deepfunneled') # Make a directory to store the data and enter it here.\n",
    "                    # We will be using a smaller dataset (LFW) than the one used in the paper (CelebA) for computational resource considerations.\n",
    "                    # Download it from http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz.\n",
    "resize_dir = os.path.join(root_dir, 'Resized_images')\n",
    "#device = -1\n",
    "#lambda_ = {'feature': 1., 'pixel': 1., 'total_variation': 1e-5}\n",
    "model_directory = os.path.join(root_dir, 'NIPS_model_ass1') # Make a directory to store the models and enter it here. Move Vgg4Layers.npz to the model directory.\n",
    "outsize = (96, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resizing and saving each portrait in the data directory\n",
    "\n",
    "'''iterator = 0\n",
    "\n",
    "for root, dirs, files in os.walk(data_directory):\n",
    "    for fname in files:\n",
    "\n",
    "        filepath = os.path.join(root, fname)\n",
    "        \n",
    "        img = cv2.imread(filepath)\n",
    "        \n",
    "        img1 = cv2.resize(img, (96, 96))\n",
    "        \n",
    "        cv2.imwrite(os.path.join(resize_dir, '%r_1.jpg' %iterator), img1)\n",
    "        \n",
    "        iterator += 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def render(img_rgb):\n",
    "    \"\"\"\n",
    "    Applies pencil sketch effect to an RGB image\n",
    "    :param img_rgb: RGB image to be processed\n",
    "    :returns: Processed RGB image\n",
    "    \"\"\"\n",
    "    img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    img_blur = cv2.GaussianBlur(img_gray, (21, 21), 0, 0)\n",
    "    img_blend = cv2.divide(img_gray, img_blur, scale=256)\n",
    "    \n",
    "    return img_blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_load(img):\n",
    "    \n",
    "    t_ = []\n",
    "    x_ = []\n",
    "    for i in img:\n",
    "        t = np.asarray(Image.open(i).convert('RGB').resize((96, 96), Image.LANCZOS), 'f').transpose(2, 0, 1)\n",
    "        x = render(np.asarray(Image.open(i).convert('RGB').resize((96, 96), Image.LANCZOS), 'f'))[None]\n",
    "        \n",
    "        t_.append(t)\n",
    "        x_.append(x)\n",
    "    \n",
    "    return np.asarray(t_), np.asarray(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_file = sorted(glob('{}/*.jpg'.format(resize_dir)))\n",
    "training_set = img_load(data_file[:int(.64 * len(data_file))])\n",
    "validation_set = img_load(data_file[int(.64 * len(data_file)) : int(.8 * len(data_file))])\n",
    "test_set = img_load(data_file[int(.8 * len(data_file) ):])\n",
    "\n",
    "batches = int(len(training_set)/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator():\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        for batch in range(batches):\n",
    "            \n",
    "            data = training_set[1][batch * BATCH_SIZE: (batch + 1) * BATCH_SIZE]\n",
    "            target = training_set[0][batch * BATCH_SIZE: (batch + 1) * BATCH_SIZE]\n",
    "                \n",
    "            yield (data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_block(y):\n",
    "    shortcut = y\n",
    "    \n",
    "    y = Conv2D(128, 3, 1, padding = 'same')(y)\n",
    "    y = BatchNormalization(axis = 0, scale = None)(y)\n",
    "    y = Activation('relu')\n",
    "    \n",
    "    y = Conv2D(128, 3, 1, padding = 'same')(y)\n",
    "    y = BatchNormalization(axis = 0, scale = None)(y)\n",
    "    y = Activation('relu')\n",
    "\n",
    "    y = layers.add([shortcut, y])\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(ZeroPadding2D(4, input_shape = (1, 96, 96), data_format='channels_first'))\n",
    "\n",
    "model.add(Conv2D(32, 9, 1))\n",
    "\n",
    "model.add(BatchNormalization(axis = 0, scale = None))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, 3, 2, padding = 'same'))\n",
    "\n",
    "model.add(BatchNormalization(axis = -1, scale = None))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(128, 3, 2, padding = 'same'))\n",
    "\n",
    "model.add(BatchNormalization(axis = 0, scale = None))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "input1 = Input(shape = (1, 96, 96))\n",
    "model1 = model(input1)\n",
    "\n",
    "res1 = residual_block(model1)\n",
    "\n",
    "res2 = residual_block(res1)\n",
    "\n",
    "res3 = residual_block(res2)\n",
    "\n",
    "res4 = residual_block(res3)\n",
    "\n",
    "res5 = residual_block(res4)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
